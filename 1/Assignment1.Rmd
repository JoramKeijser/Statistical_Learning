---
title: "Assigment 1"
author: "Joram Keijser"
date: "Monday, November 17, 2014"
output: html_document
---

```{r options, echo=FALSE}
## numbers >= 10^8 will be denoted in scientific notation,
## and rounded to 2 digits
options(scipen = 3, digits = 2)
#@
```

```{r load_packages, include=FALSE}
library(bootstrap)
install.packages('leaps', repos = "http://cran.xl-mirror.nl/")
library(MASS)
library('leaps')
```


### 1, 2 Data

Get the data, divide into test and training set.
```{r one, cache=TRUE}
Data <- read.table("http://www.timvanerven.nl/wp-content/uploads/teaching/statlearn2014/housing-p.data", header = TRUE)
Train <- Data[1:350, ]
Test <- Data[351:506, ]
```
There are 13 explanatory variables, from which 
we try to predict 1 response variable:  `MEDV`, the median value of owner-occupied homes in $1000's.We plot the each of the 13 explanatory variables against the number of `MEDV`, the median value of owner-occupied homes in $1000's. We added the single variable
least squares fit to each plot. The independent variables are, on average, influencing `MEDV` in the way we expect them to. For example: higher criminality leads to a lower price, whereas a higher number of rooms leads to an higher price.
Some plots show a nice 'continuous' relationship
between prediction and response. See e.g. `LSTAT`, the percentage lower status
of the population. Data on other variables is rather discrete of nature. 
Examples are `RAD`, an index of accessibility to radial highways and 
the binary `CHAS`. Finally, we notice some variables which do 
seem to correlate with `MEDV` in a particular way, but whose values
are lumped together around a certain value. See for example `CRIM` and `B`. 
Also included is an histogram showing the distribution of `MEDV`. 
```{r plot, fig.height=15, fig.width = 10}
par(mfrow = c(5, 3))
for(i in 1:13) {
  x <- Data[, i]
  y <- Data[, 14]
  plot(x, y, xlab = names(Data)[i], ylab = names(Data)[14])
  lm.model <- lm(y ~ x)
  lines(x, lm.model$coefficients[[1]] + lm.model$coefficients[[2]]*x, col = 'red')
}
truehist(Data$MEDV, xlab = 'MEDV', ylab = 'Percentage')
```


### 3, 4 Regression

Of the 13 explanatory variables, 3 have not-statistically significant regression coefficients in our multivariate linear model. These are `INDUS` (proportion of non-retail business acres per town), `CHAS` (boolean indicating whether the tract bounds on the Charles River) and `AGE` (proportion of owner-occupied units built prior to 1940). 
The obtained coefficients are not very surprising. The independent variables
are, on average, influencing `MEDV` in the way we expect them to. For example: higher criminality leads to a lower price, whereas a higher number of rooms leads to an higher price. In the single variable model based on all data, the variables `INDUS` and `DIS` are correlated negatively and positively, respectively. In the multivariate model
build on the training data, these relationships are switched. 
```{r, two}
lm.model <- lm(MEDV ~ ., data = Train)
summary(lm.model)
```
The plots Residuals vs fitted and Scale-location plot show that
the residuals are to some extent, but not entirely randomly distributed when plotted
against the fitted values. The Q-Q-plot shows that the residuals are neatly
normaly distributed except in the very high (and very low) range. 
A plot of Cook's distance shows that there are 3 clear outliers, but no single observation influences the regression too much. 

```{r plotML,  fig.height=8, fig.width= 10}
#http://stats.stackexchange.com/questions/58141/interpreting-plot-lm
par(mfrow = c(2, 2))
for(i in 1:4) {
  plot.lm(lm.model, which = i)
}
```

### 5. Cross-validation
The training error equals `r sum(lm.model$residuals^2)`. This corresponds to an average
squared difference between predicted and actual `MEDV` of `r mean(lm.model$residuals^2)`.
Since the variable is measured in $1000's, this corresponds to `r 20977` Dollars.
We will see that the cross validation error is higher. 
```{r, CV}
performCV <- function(Data, K, fitFUN, ...){
  set.seed(123)
  # Divide data in K folds
  folds <- sample.int(K, size = dim(Data)[[1]], replace = TRUE)
  # Vector will contain mean error of every fold
  errors <- rep(0, 5)
  for(k in 1:K) {
    # test on fold k, train on other folds:
    sprintf('Aantal: %d', sum(folds == k))
    test = Data[folds == k, ]
    train = Data[folds != k, ]    
    model <- fitFUN(MEDV ~ ., data = train)
    predict <- data.matrix(cbind(1, test[, -dim(test)[[2]]]))%*%data.matrix(model$coef)
    errors[k] <- sum((predict - test[, dim(Data)[[2]]])^2)
  }
  return(mean(errors)) #averaged over all folds
}
result <- performCV(Train, 5, lm)
result
# TO DO: 65?
```
The mean cross validation error equals `r result`. 
Averaged: `r result/65`
This is indeed higher than the training error. This does not come as a surprise,
since the cross-validation error is supposed to be a more accurate
estimate of the model's performance. 

### 6. Full subset search
```{r six}
subModels <- regsubsets(as.matrix(Train[, -14]), Train[, 14], nbest = 1, nvmax = 20)
errors <- rep(0, dim(summary(subModels)$which)[1])
for( k in 1:length(errors)) {
  select <- as.logical(summary(subModels)$which[k,])
  select[14] <- TRUE
  subset <- Train[, select]
  errors[k] <- performCV(subset, 5, lm)
}
plot(errors/65, xlab = 'Number of features', ylab = 'mean error')
summary(subModels)$which[which(errors == min(errors)), ]
# TO DO: higher nbest, plot errors against number of features.
```

Minimum of mean error: `r min(errors)/65`, achieved with all 12 features
`INDUS`. This error is slightly higher than that achieved when using all 13 features (see figure). This is not a very big surprise, since coefficient of the excluded feature
has a very high p-value in the multivariate linear model. 

### 7. Ridge

First, take a look at training error.
```{r ridgePred}
predict.ridge <- function(ridge.model, x_train, y_train, x_test) {
  intercept = -sum(ridge.model$coef * colMeans(x_train) / ridge.model$scales)
  + mean(y_train)
  ridge.coeffs = ridge.model$coef / ridge.model$scales
  return(as.matrix(x_test) %*% as.vector(ridge.coeffs) + intercept)
}

ridge.model <- lm.ridge(MEDV ~ ., data = Train)
ridge.model$coef
lm.model$coef
Ridge.predict <- predict.ridge(ridge.model, Train[, c(1:13)], Train$MEDV, Train[, c(1:13)])
```
Error equals `r sum((Ridge.predict - Train[, dim(Train)[[2]]])^2)`. Much higher
then LM error. Why oh why?
Next, find (approximately) optimal $lambda$ by cross-validation.

```{r, ridgeCV}
performCV_R <- function(Data, K){
  set.seed(123)
  # Divide data in K folds
  folds <- sample.int(K, size = dim(Data)[[1]], replace = TRUE)
  # Vector will contain mean error of every fold
  errors <- rep(0, 5)
  for(k in 1:K) {
    # test on fold k, train on other folds:
    test = Data[folds == k, ]
    train = Data[folds != k, ]    
    model <- lm.ridge(MEDV ~ ., data = train)
    predict <- predict.ridge(model, train[ ,c(1:13)], train[,14], test[, c(1:3)])
    errors[k] <- sum((predict - test[, dim(Data)[[2]]])^2)
  }
  return(mean(errors)) #averaged over all folds
}
#TO DO: refactor One method for all cross-validation.
```


