"missile", "immigration", "synfuels", "education", "superfund", "crime",
"dut-free",                 "export")
Data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data", sep = ',', col.names = attributes, na.strings = "?")
View(Data)
rm(list = c('ttributes'))
attributes <- c("party", "handicap", "water", "adoption", "physician", "el-salvador",
"religious", "sattelite", "nicaraguan",
"missile", "immigration", "synfuels", "education", "superfund", "crime",
"dut-free", "export")
Data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data", sep = ',', col.names = attributes, na.strings = "?")
getLosses <- function(STEP){
N = dim(Train)[1]
sampleSizes <- seq(1, N, by = STEP)
## Naive Bayes:
getNBloss <- function(Train, Test) {
lossNB <- data.frame(matrix(ncol = length(alpha), nrow = length(sampleSizes)))
colnames(lossNB) <- alpha
rownames(lossNB) <- sampleSizes
for(n in sampleSizes) {
for( k in 1:length(alpha)) {
tempTrain <- Train[1:n, ]
NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
prediction <- predict(NB, newdata = Test)
lossNB[n, k] <- mean(Test$party != prediction)
}
}
return(lossNB)
}
# Logistic regression:
tryGetLoss <- function(tempTrain) {
lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
lr.prob = predict(lr, newdata = Test, type = "response")
lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
mean(lr.pred != Test$party)
}
getLRloss <- function(Train, Test) {
lossLR <- rep(NA, length(sampleSizes))
for(n in sampleSizes) {
tempTrain <- Train[1:n, ]
lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
}
return(lossLR)
}
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
return(cbind(getNBloss(Train, Test),
LRloss = getLRloss(Train, Test),
LR2loss = getLRloss(Train[, 1:3], Test[1:3])))
}#getLosses
install.packages("e1071", repos = "http://cran.xl-mirror.nl/")
library("e1071")
install.packages("zoo", repos = "http://cran.xl-mirror.nl/")
library("zoo
test <- replicate(10, getLosses(STEP = 4), simplify = FALSE)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance)
)
""
"
install.packages("zoo", repos = "http://cran.xl-mirror.nl/")
library("zoo")
test <- replicate(10, getLosses(STEP = 4), simplify = FALSE)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance)
Data <- na.roughfix(Data)
set.seed(14122014)
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
test <- replicate(10, getLosses(STEP = 4), simplify = FALSE)
alpha = c(1, 0.1, 10)
test <- replicate(10, getLosses(STEP = 4), simplify = FALSE)
test <- replicate(10, getLosses(STEP = 10), simplify = FALSE)
View(test)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance)
View(meanPerformance)
plot(meanPerformance[,1])
plot(meanPerformance[,1], type = 'l ')
plot(meanPerformance[,1], type = 'l')
plot(meanPerformance[, 1], type = 'l',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
plot(meanPerformance[, 1], type = 'l',
xlab = "Size of training set", ylab = "mean 0/1-loss", col = '133')
plot(meanPerformance[, 1], type = 'l',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(26, 33, 142, 524)
for(i in 2:5) {
lines(meanPerformance[, i], type = 'l', col = colors[i-1])
}
plot(c(0, 200), c(0, 0.4), type = 'l',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
plot(meanPerformance[, 1], type = 'l',
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'o', col = colors[i-1])
}
plot(c(0, 200), c(0, 0.4), type = 'l',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'o', col = colors[i-1])
}
plot(c(0, 200), c(0, 0.4), type = 'l',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
plot(c(0, 200), c(0, 0.4), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'l', col = colors[i-1])
}
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'l', col = colors[i-1])
}
legend(150,0.4, # places a legend at the appropriate place c("Health","Defense"), # puts text in the legend
lty=rep(1, 5), # gives the legend appropriate symbols (lines)
lwd=rep(2.5, 5),col=colors) # gives the legend lines the correct color and width
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'l', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
sum(abs(meanPerformance[,1]-meanPerformance[,2]))
?numeric
ind = seq(from = 1, to = length(meanPerformance), by = 20)
ind
meanPerformance[ind]
meanPerformance[ind. ]
View(meanPerformance[ind, ])
meanPerformance[ind, ]
lenght(meanPerformance)
length(meanPerformance)
size(meanPerformance)
dim(meanPerformance)
ind = seq(from = 1, to = dim(meanPerformance)[1], by = 20)
View(meanPerformance[ind, ])
alpha = c(1, 0.1, 10)
getNBloss <- function(Train) {
N <- dim(Train)[1]
lossNB <- data.frame(matrix(ncol = length(alpha), nrow = N))
colnames(lossNB) <- alpha
rownames(lossNB) <- 1:N
for(n in 1:N) {
for( k in 1:length(alpha)) {
tempTrain <- Train[1:n, ]
NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
prediction <- predict(NB, newdata = Test)
lossNB[n, k] <- mean(Test$party != prediction)
}
}
return(lossNB)
}
lossNB <- getNBloss(Train)
lossNB <- data.frame(matrix(ncol = 3, nrow = 1))
NB <- naiveBayes(party ~., data = Train, alpha = 1)
prediction <- predict(NB, newdata = Test)
lossNB[1,1] <- mean(Test$party!= predictoin)
lossNB[1,1] <- mean(Test$party!= prediction)
LossNB
lossNB
NB <- naiveBayes(party ~., data = Train, alpha = 0.1)
prediction <- predict(NB, newdata = Test)
lossNB[1,2] <- mean(Test$party!= prediction)
lossNB
NB <- naiveBayes(party ~., data = Train, alpha = 10)
prediction <- predict(NB, newdata = Test)
lossNB[1,3] <- mean(Test$party!= prediction)
lossNB
NB <- naiveBayes(party ~., data = Train, alpha = 200)
prediction <- predict(NB, newdata = Test)
mean(Test$party!= prediction)
NB <- naiveBayes(party ~., data = Train, alpha = 500)
NB <- naiveBayes(party ~., data = Train, alpha = -10)
NB <- naiveBayes(party ~., data = Train, alpha = 10^4)
?NB
?naiveBayes
title(main = 'Test error for all methods')
title(main = 'Test error')
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
title(main = 'Test error')
NB
NB <- naiveBayes(party ~., data = Train, alpha = 1)
prediction <- predict(NB, newdata = Test)
mean(Test$party != predictoin)
mean(Test$party != prediction)
prediction <- predict(NB, newdata = Ttrain)
prediction <- predict(NB, newdata = Train)
mean(Test$party != prediction)
mean(Train$party != prediction)
View(prediction)
plotPerformance(meanPerformance, type) {
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(TestPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
if(type == 'test')
title(main = 'Test error')
else
title(main = 'Training error')
}
plotPerformance(meanPerformance, type) {
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
if(type == 'test')
title(main = 'Test error')
else
title(main = 'Training error')
}
plotPerformance(meanPerformance, dataset) {
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
if(dataset == 'test')
title(main = 'Test error')
else
title(main = 'Training error')
}
dataset = 'test'
dataset
plotPerformance(meanPerformance, dataset) {
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
if(dataset == 'test')
title(main = 'Test error')
else
title(main = 'Training error')
}
plotPerformance(meanPerformance) {
plot(c(0, 200), c(0, 0.5), type = 'nl',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), lwd=rep(2.5, 5),col=colors)
}
plotPerformance(meanPerformance) {
plot(c(0, 200), c(0, 0.5), type = 'n',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5),
lwd=rep(2.5, 5), col=colors)
}
plotPerformance <- function(meanPerformance) {
plot(c(0, 200), c(0, 0.5), type = 'n',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5),
lwd=rep(2.5, 5), col=colors)
}
plotPerformance(meanPerformance)
lotPerformance <- function(meanPerformance, type) {
plot(c(0, 200), c(0, 0.5), type = 'n',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5),
lwd=rep(2.5, 5), col=colors)
if( type == 'test') {
title(main = 'Test errors')
}
else {
title(main = 'Training errors')
}
}
plotPerformance(meanPerformance, train)
plotPerformance(meanPerformance, type = train)
plotPerformance(meanPerformance, type = 'train')
lotPerformance <- function(meanPerformance, type) {
plot(c(0, 200), c(0, 0.5), type = 'n',
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c(1, 26, 33, 142, 524)
for(i in 1:5) {
lines(meanPerformance[, i], type = 'p', col = colors[i-1])
}
legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5),
lwd=rep(2.5, 5), col=colors)
if( type == 'test') {
title(main = 'Test errors')
}
else {
title(main = 'Training errors')
}
}
getLosses <- function(STEP){
N = dim(Train)[1]
sampleSizes <- seq(1, N, by = STEP)
## Naive Bayes:
getNBloss <- function(Train, Test) {
lossNB <- data.frame(matrix(ncol = length(alpha), nrow = length(sampleSizes)))
colnames(lossNB) <- alpha
rownames(lossNB) <- sampleSizes
for(n in sampleSizes) {
for( k in 1:length(alpha)) {
tempTrain <- Train[1:n, ]
NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
prediction <- predict(NB, newdata = Test)
lossNB[n, k] <- mean(Test$party != prediction)
}
}
return(lossNB)
}
# Logistic regression:
tryGetLoss <- function(tempTrain) {
lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
lr.prob = predict(lr, newdata = Test, type = "response")
lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
mean(lr.pred != Test$party)
}
getLRloss <- function(Train, Test) {
lossLR <- rep(NA, length(sampleSizes))
for(n in sampleSizes) {
tempTrain <- Train[1:n, ]
lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
}
return(lossLR)
}
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
return(cbind(getNBloss(Train, Test),
LRloss = getLRloss(Train, Test),
LR2loss = getLRloss(Train[, 1:3], Test[1:3])))
}#getLosses
getLosses <- function(STEP, dataset){
# input: number of steps STEP to skip,
# dataset (test or train) for which losses should be calculated
N = dim(dataset)[1]
sampleSizes <- seq(1, N, by = STEP)
## Naive Bayes:
getNBloss <- function(Train, Test) {
lossNB <- data.frame(matrix(ncol = length(alpha), nrow = length(sampleSizes)))
colnames(lossNB) <- alpha
rownames(lossNB) <- sampleSizes
for(n in sampleSizes) {
for( k in 1:length(alpha)) {
tempTrain <- Train[1:n, ]
NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
prediction <- predict(NB, newdata = dataset)
lossNB[n, k] <- mean(dataset$party != prediction)
}
}
return(lossNB)
}
# Logistic regression:
tryGetLoss <- function(tempTrain) {
lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
lr.prob = predict(lr, newdata = dataset, type = "response")
lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
mean(lr.pred != dataset$party)
}
getLRloss <- function(Train, Test) {
lossLR <- rep(NA, length(sampleSizes))
for(n in sampleSizes) {
tempTrain <- Train[1:n, ]
lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
}
return(lossLR)
}
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
return(cbind(getNBloss(Train, Test),
LRloss = getLRloss(Train, Test),
LR2loss = getLRloss(Train[, 1:3], Test[1:3])))
}#getLosses
calculateMeanPerformance <- function(replicates = 10, dataset){
# input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(replicateds, getLosses(STEP = 10, test), simplify = FALSE)
meanPerformance <- Reduce("+", dataset) / length(dataset) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
mP <- calculateMeanPerformance(replicates = 1, dataset = Test)
calculateMeanPerformance <- function(replicates = 10, dataset){
# input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(replicates, getLosses(STEP = 10, test), simplify = FALSE)
meanPerformance <- Reduce("+", dataset) / length(dataset) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
mP <- calculateMeanPerformance(replicates = 1, dataset = Test)
calculateMeanPerformance <- function(reps = 10, dataset){
# input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(reps, getLosses(STEP = 10, test), simplify = FALSE)
meanPerformance <- Reduce("+", dataset) / length(dataset) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
mP <- calculateMeanPerformance(reps = 1, dataset = Test)
mP <- calculateMeanPerformance(reps = 21, dataset = Test)
calculateMeanPerformance <- function(reps = 10, dataset){
# input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(reps, getLosses(STEP = 10, dataset), simplify = FALSE)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
mP <- calculateMeanPerformance(reps = 1, dataset = Test)
View(mP)
mP <- calculateMeanPerformance(reps = 2, dataset = Test)
View(mP)
dim(mp)
dim(mP)
TestPerformance <- calculateMeanPerformance(dataset = Test)
calculateMeanPerformance <- function(reps = 10, dataset){
# input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(reps, getLosses(STEP = 12, dataset), simplify = FALSE)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
TestPerformance <- calculateMeanPerformance(reps = 1, dataset = Test)
View(TestPerformance)
TrainPerformance <- calculateMeanPerformance(dataset = Train)
TrainPerformance <- calculateMeanPerformance(reps = 1, dataset = Train)
View(TrainPerformance)
getLosses(step = 10, dataset = Train)
getLosses <- function(STEP, dataset){
# input: number of steps STEP to skip,
# dataset (test or train) for which losses should be calculated
N = dim(dataset)[1]
sampleSizes <- seq(1, N, by = STEP)
## Naive Bayes:
getNBloss <- function(Train, Test) {
lossNB <- data.frame(matrix(ncol = length(alpha), nrow = length(sampleSizes)))
colnames(lossNB) <- alpha
rownames(lossNB) <- sampleSizes
for(n in sampleSizes) {
for( k in 1:length(alpha)) {
tempTrain <- Train[1:n, ]
NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
prediction <- predict(NB, newdata = dataset)
lossNB[n, k] <- mean(dataset$party != prediction)
}
}
return(lossNB)
}
# Logistic regression:
tryGetLoss <- function(tempTrain) {
lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
lr.prob = predict(lr, newdata = dataset, type = "response")
lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
mean(lr.pred != dataset$party)
}
getLRloss <- function(Train, Test) {
lossLR <- rep(NA, length(sampleSizes))
for(n in sampleSizes) {
tempTrain <- Train[1:n, ]
lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
}
return(lossLR)
}
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
return(cbind(getNBloss(Train, Test),
LRloss = getLRloss(Train, Test),
LR2loss = getLRloss(Train[, 1:3], Test[1:3])))
}#getLosses
getLosses(STEP = 10, dataset = Train)
dim(Train)
dim(Test)
mp <- calculateMeanPerformance(reps = 1, dataset = Test)
View(mp)
plotPerformance(mp)
title(main = 'Training errors')
