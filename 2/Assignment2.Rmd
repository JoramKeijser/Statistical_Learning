w?---
title: 'Assignment 2: Classification'
author: "Joram Keijser"
date: "Wednesday, December 10, 2014"
output: html_document
---

```{r package, include=FALSE}
install.packages("randomForest", repos = "http://cran.xl-mirror.nl/")
library("randomForest")
```

### 0. Introduction

First, download the data. 
```{r download}
attributes <- c("party", "handicap", "water", "adoption", "physician", "el-salvador", 
                "religious", "sattelite", "nicaraguan", 
                "missile", "immigration", "synfuels", "education", "superfund", "crime", 
                "dut-free", "export")
Data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data", sep = ',', col.names = attributes, na.strings = "?")
```

There are `r sum(is.na(Data))` missing values. We replace the missing values
by the most occuring value in the particular. For example, if
most votes on the `handicap` topic were a yes, than the missing votes on
this topic will taken to be yes. This is done by the `na.roughfix` function. 
```{r missing}
Data <- na.roughfix(Data)
```

```{r meanVotes, include=FALSE}
REP <- Data$party == "republican"
DEM <- Data$party == "democrat"
meanVotes <- data.frame(matrix(ncol = dim(Data)[2], nrow = 2))
names(meanVotes) <- attributes
meanVotes[1,1] <- "Republican"
meanVotes[2,1] <- "Democrat"
for( k in 2:dim(Data)[2] ) {
  meanVotes[1, k] <- mean(Data[REP, k] == "y")
  meanVotes[2, k] <- mean(Data[DEM, k] == "y")
}
```
Next, we will take a first look at the data. There are `r sum(REP)` republicans
and `r sum(DEM)` democrats. On average, both parties voted 
in favour approximately half of the time. 
The biggest differences in voting behaviour are observed on the topic
Physician. Only `r 100*mean(Data[DEM, "physician"] == "y")`percent
of the democrats voted in favour. On the other hand, `r 100*mean(Data[REP, "physician"] == "y")` of the republicans were in favour. This indicates that it should be possible
to make accurate predictions, based only on the votin behaviour on this single topic.
```{r plots1}
## plot mean number of yes's per party
par(las=2) # make label text perpendicular to axis
par(mar=c(5,8,4,2)) # increase y-axis margin.
colours = c("red", "darkblue")
barplot(100*as.matrix(meanVotes[,-1]), beside = TRUE, col = colours, horiz=T, 
        xlim=c(0,100), xlab="Percentage of party voted in favour", ylab="Topic")
legend("bottomright", legend = c("Repulicans", "Democrats"), fill = colours)
#sort(apply(meanVotes, 2, function(x){ max(x)/min(x)}), decreasing = TRUE)
sort(apply(meanVotes[,-1], 2, function(x){ 
max(as.vector(x, mode = "numeric"))/min(as.vector(x, mode = "numeric"))}), 
decreasing = TRUE)
```

Finallly, divide the data in a training and test set.
```{r test/train}
set.seed(14122014)
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
```

### 1. Naive Bayes classification

```{r NBpackage, include = FALSE}
install.packages("e1071", repos = "http://cran.xl-mirror.nl/")
library("e1071")
```
Train Naive Bayes classifier for different values of alpha.
```{r NB, cache=TRUE}
alpha = c(1, 0.1, 10)
getNBloss <- function(Train) {
  N <- dim(Train)[1]
  lossNB <- data.frame(matrix(ncol = length(alpha), nrow = N))
  colnames(lossNB) <- alpha
  rownames(lossNB) <- 1:N
  for(n in 1:N) { 
    for( k in 1:length(alpha)) {
      tempTrain <- Train[1:n, ]
      NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
      prediction <- predict(NB, newdata = Test)
      lossNB[n, k] <- mean(Test$party != prediction)
    }
  }
  return(lossNB)
}
lossNB <- getNBloss(Train)
```
Mean 0/1-loss is approximately the same when using all training data, namely
`r lossNB[length(lossNB), 1]`. Results are approximately the same for
all $alpha$'s.

Comparable results for $n=2$ and $n=218$! Why? Maybe highly predicting
features. Try with and without physician etc. 

```{r plotNBloss}
plot(lossNB[, 1],  xlab="Size of training set", ylab = "Mean 0/1-loss")
```

### 2. Logistic Regression
```{r logr0, warning=FALSE}
lr <- glm(party ~., data = Train, family = binomial("logit"))
summary(lr)
```

```{r logr, warning=FALSE}
tryGetLoss <- function(tempTrain) {
  lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
  lr.prob = predict(lr, newdata = Test, type = "response")
  lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
  mean(lr.pred != Test$party)
}
getLRloss <- function(Train) {
  lossLR <- rep(1, length(dim(Train)[1]))
  for(n in 1:218) { 
    tempTrain <- Train[1:n, ]
    lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
  }
  return(lossLR)
}
lossLR <- getLRloss(Train)
plot(lossLR[lossLR < 1], xlab = "size of training set", ylab = "mean 0/1-loss")
#TO DO: confusion matrix, Naive Bayes line. 
```
When using all `r dim(Train)[1]` training examples, it's 
mean 0/1-loss equals `r tail(lossLR, n = 1)`. Logistic regression outperforms 
Naive Bayes from approximately n = `r which(lossNB > lossLR)[1]`.


### 3. Logistic Regression on first attributes
In this case, the result is nearly not as good as when regressing 
on all variables. It isn't even as good as Naive Bayes. Interestingly,
performance does not improve after seeing approximately $n=75$ training examples.
Perhaps all useful possible combinations of the 2 features and the class have been seen
at this point. 
```{r logr2, warning=FALSE}
lr <- glm(party ~., data = Train[1:3], family = binomial("logit"))
summary(lr)
```

```{r logr4, warning=FALSE}
getLR2loss <- function(Train) {
  lossLR2 <- numeric(dim(Train)[1])
  for(n in 1:218) { 
    length(lossLR2)
    tempTrain <- Train[1:n, 1:3]
    lossLR2[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) {1}, 
                           warning = function(c) {1})
  }
  return(lossLR2)
}
lossLR2 <- getLR2loss(Train)
plot(lossLR2[lossLR2 < 1], xlab = "size of training set", ylab = "mean 0/1-loss")
```

### 4. Comparison
Make wrapper method `getLosses` for three algorithms (Naive Bayes, logistic regression
and subset logistic regression). 

```{r together, include=FALSE}
getLosses <- function(STEP, dataset){
  # input: number of steps STEP to skip,
  # dataset (test or train) for which losses should be calculated
  N = dim(dataset)[1]
  sampleSizes <- seq(1, N, by = STEP)
  ## Naive Bayes:
  getNBloss <- function(Train, Test) {
    lossNB <- data.frame(matrix(ncol = length(alpha), nrow = length(sampleSizes)))
    colnames(lossNB) <- alpha
    rownames(lossNB) <- sampleSizes
    for(n in sampleSizes) { 
      for( k in 1:length(alpha)) {
        tempTrain <- Train[1:n, ]
        NB <- naiveBayes(party ~., data = tempTrain, alpha = alpha[k])
        prediction <- predict(NB, newdata = dataset)
        lossNB[n, k] <- mean(dataset$party != prediction)
      }
    }
    return(lossNB)
  }
  
  # Logistic regression:
  tryGetLoss <- function(tempTrain) {
    lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
    lr.prob = predict(lr, newdata = dataset, type = "response")
    lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
    mean(lr.pred != dataset$party)
  }
  getLRloss <- function(Train, Test) {
    lossLR <- rep(NA, length(sampleSizes))
    for(n in sampleSizes) { 
      tempTrain <- Train[1:n, ]
      lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
    }
    return(lossLR)
  }
  
  trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
  Train <- Data[trainIndex, ]
  Test <- Data[-trainIndex, ]
  
  
  return(cbind(getNBloss(Train, Test), 
               LRloss = getLRloss(Train, Test), 
               LR2loss = getLRloss(Train[, 1:3], Test[1:3])))
}#getLosses
```

```{r zoopackge, include=FALSE}
install.packages("zoo", repos = "http://cran.xl-mirror.nl/")
library("zoo")
``` 
```{r comparisonfunction, cache=TRUE}
calculateMeanPerformance <- function(reps = 10, dataset){ 
  # input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(reps, getLosses(STEP = 12, dataset), simplify = FALSE)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
```
```{r Testcomparison, warning=FALSE, cache=TRUE}
TestPerformance <- calculateMeanPerformance(dataset = Test)
```

```{r TestcomparisonPlot}
plotPerformance <- function(meanPerformance) {
  plot(c(0, 200), c(0, 0.5), type = 'n', 
       xlab = "Size of training set", ylab = "mean 0/1-loss",)
  colors = c(1, 26, 33, 142, 524)
  for(i in 1:5) {
    lines(meanPerformance[, i], type = 'p', col = colors[i-1])
  }
  legend(150,0.4, c("NB1", "NB2", "NB3", "LG1", "LG2"), lty=rep(1, 5), 
         lwd=rep(2.5, 5), col=colors)
}
plotPerformance(TestPerformance)
title(main = 'Tets errors')
```
### 5. Analysis
All three implementations of Naive Bayes perform exactly the same. 
The smoothing parameters does not have the slightest influence. 
For small training sizes, Naive Bayes outperforms logistic regression.
From approximately $n = 75$, logistic regression beats Naive Bayes. 
This is in accordance with the theory (Andrew Y. Ng, Michael Jordan: On Discriminative vs. Generative Classifiers: A comparison of logistic regression and Naive Bayes, NIPS 2001). Logistic regression done on the first features is clearly undesirable. 
Finally, all methods roughly reach their best performance for a training set size 
of 75 examples. After that, performance improves only slightly. 

```{r Trainperformance, warning=FALSE, cache=TRUE}
TrainPerformance <- calculateMeanPerformance(dataset = Train)
```
```{r TtraincomparisonPlot}
plotPerformance(TrainPerformance)
title(main = 'Training errors')
```
VRAGEN:
1. Laplace smoothing maakt helemaal geen vershil. Waarom niet?
2. Verschil test/training error (overfitting?)
### 6. BONUS!!!:D
### 6. Subset search.
