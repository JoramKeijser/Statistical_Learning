---
title: 'Assignment 2: Classification'
author: "Joram Keijser"
date: "Wednesday, December 10, 2014"
output: html_document
---

```{r package, include=FALSE}
install.packages("randomForest", repos = "http://cran.xl-mirror.nl/")
library("randomForest")
```

### 0. Introduction

First, download the data. 
```{r download}
attributes <- c("party", "handicap", "water", "adoption", "physician", "el-salvador", 
                "religious", "sattelite", "nicaraguan", 
                "missile", "immigration", "synfuels", "education", "superfund", "crime", 
                "dut-free", "export")
Data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data", sep = ',', col.names = attributes, na.strings = "?")
```

There are `r sum(is.na(Data))` missing values. We replace the missing values
by the most occuring value in the particular column. This is done by the `na.roughfix` function. 
We do this seperately for the two parties. For example, if
most democrats voted yes on the topic of `handicap`, a missing democrat vote
will taken to be yes as well.
```{r missing}
#Data <- na.roughfix(Data)
Data[Data[,1] == "democrat", ] <- na.roughfix(Data[Data[,1] == "democrat", ])
Data[Data[,1] == "republican", ] <- na.roughfix(Data[Data[,1] == "republican", ])
```

```{r meanVotes, include=FALSE}
REP <- Data$party == "republican"
DEM <- Data$party == "democrat"
meanVotes <- data.frame(matrix(ncol = dim(Data)[2], nrow = 2))
names(meanVotes) <- attributes
meanVotes[1,1] <- "Republican"
meanVotes[2,1] <- "Democrat"
for( k in 2:dim(Data)[2] ) {
  meanVotes[1, k] <- mean(Data[REP, k] == "y")
  meanVotes[2, k] <- mean(Data[DEM, k] == "y")
}
```
Next, we will take a first look at the data. There are `r sum(REP)` republicans
and `r sum(DEM)` democrats. On average, both parties voted 
in favour approximately half of the time. 
The biggest differences in voting behaviour are observed on the topic
Physician. Only `r 100*mean(Data[DEM, "physician"] == "y")`percent
of the democrats voted in favour. On the other hand, `r 100*mean(Data[REP, "physician"] == "y")` of the republicans were in favour. This indicates that it should be possible
to make accurate predictions, based only on the voting behaviour on this single topic.
```{r plots1}
## plot mean number of yes's per party
par(las=2) # make label text perpendicular to axis
par(mar=c(5,8,4,2)) # increase y-axis margin.
colours = c("red", "darkblue")
barplot(100*as.matrix(meanVotes[,-1]), beside = TRUE, col = colours, horiz=T, 
        xlim=c(0,100), xlab="Percentage of party voted in favour", ylab="Topic")
legend("bottomright", legend = c("Repulicans", "Democrats"), fill = colours)
sort(apply(meanVotes[,-1], 2, function(x){ 
max(as.vector(x, mode = "numeric"))/min(as.vector(x, mode = "numeric"))}), 
decreasing = TRUE)
```

Finallly, divide the data in a training and test set.
```{r test/train}
set.seed(14122014)
trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
Train <- Data[trainIndex, ]
Test <- Data[-trainIndex, ]
```

### 1. Naive Bayes classification

```{r NBpackage, include = FALSE}
install.packages("e1071", repos = "http://cran.xl-mirror.nl/")
library("e1071")
```
Train Naive Bayes classifier for different values of alpha.
```{r NB, cache=TRUE}
alpha = c(1, 0.1, 10)
getNBloss <- function(lossData = Test) {
  N <- dim(Train)[1]
  lossNB <- data.frame(matrix(ncol = length(alpha), nrow = N))
  colnames(lossNB) <- alpha
  rownames(lossNB) <- 1:N
  for(n in 1:N) { 
    for( k in 1:length(alpha)) {
      tempTrain <- Train[1:n, ]
      NB <- naiveBayes(party ~., data = tempTrain, laplace = alpha[k])
      prediction <- predict(NB, newdata = lossData)
      lossNB[n, k] <- mean(lossData$party != prediction)
    }
  }
  return(lossNB)
}
lossNB <- getNBloss(Test)
```

```{r TrainNB, include=FALSE, cache=TRUE}
trainLoss <- getNBloss(Train)
pred <- predict(NB, newdata = Test, type = "raw")
```
Mean 0/1-losses when using all training data equals
`r tail(lossNB, n = 1)` for $\alpha = 0.1, 1, 10$. The training losses are all equal to `r tail(trainLoss[,1], n= 1)`. Hence, the difference with the test error is neglectable and overfitting does not seem 
to be an issue. 
The difference, however small, increases with $\alpha$. Any overfitting 
would hence occur for $\alpha = 10$. 
Plotting the errors versus the size of the training set shows that the different values of
$\alpha$ matter for small sample sizes. There is virtually no difference between
$\alpha = 0.1$ and $\alpha = 1$. The error is slightly higher for $\alpha = 10$. 
This difference vanishes for approximately $n=120$ and up. This is as might be expected,
since adding 10 virtual counts does not matter when the actual count is high. 


```{r plotNBloss, include=FALSE, cache=TRUE}
plot(c(0, 220), c(0, 0.6), type = 'n', 
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c("black", "blue", "red")
for(i in 1:3) { # 1 time NB
lines(lossNB[i], type = 'l', col = colors[i])
}
legend(150,0.6, c("0.1", "1", "1"), lty=rep(1, 5), 
lwd=rep(2.5, 5), col=colors)
title(main = 'Training errors')
```

```{r plotNBTESTloss, include=FALSE,cache=TRUE}
plot(c(0, 220), c(0, 0.6), type = 'n', 
xlab = "Size of training set", ylab = "mean 0/1-loss",)
colors = c("black", "blue", "red")
for(i in 1:3) { # 1 time NB
lines(trainLoss[i], type = 'l', col = colors[i])
}
legend(150,0.6, c("0.1", "1", "1"), lty=rep(1, 5), 
lwd=rep(2.5, 5), col=colors)
title(main = 'Test errors')
```

When using all data, `r round(1000*mean(pred[,2] > 0.9 | pred[,2] < 0.1))/10` percent of 
the probability estimates are 'extreme': higher than 0.9 or lower than 0.1.

```{r NBprobs, cache=TRUE}
hist(pred[,1], xlab = "predicted probability", main = "Naive Bayes")
```
### 2. Logistic Regression
Next, we turn to logistic regression. 
```{r logr0, warning=FALSE}
#lr <- glm(party ~., data = Train, family = binomial("logit"))
#summary(lr)
```

```{r logr, warning=FALSE, cache=TRUE}
tryGetLoss <- function(tempTrain) {
  lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
  lr.prob = predict(lr, newdata = Test, type = "response")
  lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
  mean(lr.pred != Test$party)
}
getLRloss <- function(Train) {
  lossLR <- rep(1, length(dim(Train)[1]))
  for(n in 1:218) { 
    tempTrain <- Train[1:n, ]
    lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
  }
  return(lossLR)
}
lossLR <- getLRloss(Train)
plot(lossLR[lossLR < 1], xlab = "size of training set", ylab = "mean 0/1-loss")
#TO DO: confusion matrix, Naive Bayes line. 
```

```{r LRtrainLoss, include=FALSE, cache=TRUE}
lr <- glm(party ~., data = Train, family = binomial("logit"))
lr.prob = predict(lr, type = "response")
lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
trainLoss <- mean(lr.pred != Train$party) 
```
When using all `r dim(Train)[1]` training examples, the 
mean 0/1-loss equals `r tail(lossLR, n = 1)`. The error on the training set
is approximately half of that, `r trainLoss`. This indicates a some overfitting. 
A total of `r round(1000*mean(lr.prob > 0.9 | lr.prob < 0.1))/10` percent of the cases is predicted
with at least 0.9 certainty.

```{r lgprobs, cache=TRUE}
hist(lr.prob, xlab = "predicted probability", main = "Logistic regression")
``` 


### 3. Logistic Regression on two attributes
Finally, we regress only on the first two attributes, `r names(Train[,2])` and `r names(Train[,3])`. 
```{r logr2, warning=FALSE}
lr2 <- glm(party ~., data = Train[1:3], family = binomial("logit"))
summary(lr)
```

```{r logr4, warning=FALSE, cache=TRUE}
getLR2loss <- function(Train) {
  lossLR2 <- numeric(dim(Train)[1])
  for(n in 1:218) { 
    length(lossLR2)
    tempTrain <- Train[1:n, 1:3]
    lossLR2[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) {1}, 
                           warning = function(c) {1})
  }
  return(lossLR2)
}
lossLR2 <- getLR2loss(Train)
plot(lossLR2[lossLR2 < 1], xlab = "size of training set", ylab = "mean 0/1-loss")
```
```{r, include=FALSE, cache=TRUE}
lr2.prob = predict(lr2, type = "response")
lr2.pred = ifelse(lr2.prob > 0.5, "republican", "democrat")
TrainLoss <- mean(lr2.pred != Train$party)
```

In this case, the result is nearly not as good as when regressing 
on all variables. It isn't even as good as Naive Bayes. Interestingly,
performance does not improve after seeing approximately $n=75$ training examples.
Perhaps all useful possible combinations of the 2 features and the class have been seen
at this point. The test and trianing error equal `r tail(lossLR2, n = 1)` and `r TrainLoss`, 
respectively. It is very surprising that the training error is the highest of the two.
The difference is not very big, but we are definitely not overfitting here. This
is actually not very surprising, since the model is very simple. 
Now, `r round(1000*mean(lr2.prob > 0.9 | lr2.prob < 0.1))/10` cases are predicted
with certainty 0.9. 

```{r lg2robs, include=FALSE, cache=TRUE}
hist(lr2.prob, xlab = "predicted probability", main = "Logistic regression on, 2 attributes")
``` 

### 4. Comparison
We write a wrapper method `getLosses` for calculating the losses of the three algorithms (Naive Bayes, logistic regression and subset logistic regression) all at once. Then, we 
calculate the mean performance over multiple runs. We average the result to obtain a somewhat
smoother plot. In the resulting figure, the three Naive Bayes versions are indistinguishable from eachother.

```{r together, include=FALSE}
getLosses <- function(STEP){
  N = dim(Train)[1]
  sampleSizes <- seq(1, N, by = STEP)
  ## Naive Bayes:
  getNBloss <- function(Train, Test) {
    lossNB <- data.frame(matrix(ncol = length(alpha), nrow = length(sampleSizes)))
    colnames(lossNB) <- alpha
    rownames(lossNB) <- sampleSizes
    for(n in sampleSizes) { 
      for( k in 1:length(alpha)) {
        tempTrain <- Train[1:n, ]
        NB <- naiveBayes(party ~., data = tempTrain, laplace = alpha[k])
        prediction <- predict(NB, newdata = Test)
        lossNB[n, k] <- mean(Test$party != prediction)
      }
    }
    return(lossNB)
  }

  # Logistic regression:
  tryGetLoss <- function(tempTrain) {
    lr <- glm(party ~., data = tempTrain, family = binomial("logit"))
    lr.prob = predict(lr, newdata = Test, type = "response")
    lr.pred = ifelse(lr.prob > 0.5, "republican", "democrat")
    mean(lr.pred != Test$party)
  }
  getLRloss <- function(Train, Test) {
    lossLR <- rep(NA, length(sampleSizes))
    for(n in sampleSizes) { 
      tempTrain <- Train[1:n, ]
      lossLR[n] <- tryCatch(tryGetLoss(tempTrain), error = function(c) 1)
    }
    return(lossLR)
  }

  trainIndex <- sample.int(dim(Data)[1], size = round(dim(Data)[1]/2))
  Train <- Data[trainIndex, ]
  Test <- Data[-trainIndex, ]


  return(cbind(getNBloss(Train, Test), 
               LRloss = getLRloss(Train, Test), 
               LR2loss = getLRloss(Train[, 1:3], Test[1:3])))
}#getLosses
```

```{r zoopackge, include=FALSE}
install.packages("zoo", repos = "http://cran.xl-mirror.nl/")
library("zoo")
``` 
```{r comparisonfunction, cache=TRUE}
calculateMeanPerformance <- function(reps = 10){ 
  # input: dataset of wich loss should be calculated. Either test or train.
test <- replicate(reps, getLosses(STEP = 12), simplify = FALSE)
meanPerformance <- Reduce("+", test) / length(test) #average
meanPerformance <- na.approx(meanPerformance) # interpolate
return(meanPerformance)
}
```
```{r Testcomparison, warning=FALSE, cache=TRUE}
TestPerformance <- calculateMeanPerformance()
```

```{r TestcomparisonPlot, cache=TRUE}
plotPerformance <- function(meanPerformance) {
  plot(c(0, 220), c(0, 0.6), type = 'n', 
       xlab = "Size of training set", ylab = "mean 0/1-loss",)
  colors = c("black", "blue", "red", "yellow", "orange")
  for(i in 1:5) { # 1 time NB
    lines(meanPerformance[, i], type = 'l', col = colors[i])
  }
  legend(150,0.6, c("NB 0.1", "NB 1", "NB 10",  "LG", "LG 2"), lty=rep(1, 5), 
         lwd=rep(2.5, 5), col=colors)
}
plotPerformance(TestPerformance)
title(main = 'Test errors')
```


### 5. Discussion
The three different implementations of Naive Bayes have a comparable
performance for larger sample sizes. For small samples sizes, 
a relatively small value of $\alpha$ seems preferable over $\alpha =10$.  
For small training sizes, Naive Bayes outperforms logistic regression.
Interestingly, the Naive Bayes error does not decrease
ofter given only about 25 training examples. It takes 
approximately 75 examples for logistic regression to Naive Bayes. 
Long after that, logistic regression is still improving, 
whereas Naive Bayes does not benefit from the new data. 
This is in accordance with the theory (Andrew Y. Ng, Michael Jordan: On Discriminative vs. Generative Classifiers: A comparison of logistic regression and Naive Bayes, NIPS 2001). Logistic regression done on the first features is clearly undesirable. It has a very large error compared to the other
methods. The error levels off after $n=75$ examples. Much faster than 
logistic regression on all features, but still slower than Naive Bayes. 
This is probably due to the fact that after this point, all is learned wat can be learned
from the data. There are no new combinations to be found which would enable a better separation of the classes.

The next question is, do the models overfit? 
For logistic regression, the error on the training set is lower than the test error. This indicates that 
this model indeed suffers from overfitting. For Naive Bayes, there is only a very small difference between
the two errors and for logistic regression, the training error is actually a little bigger
than the test error. This suggest that these models, especially the latter, are too simple, and would benefit from adding some complexity. In the case of logistic regression, this is obvious: we 
would do better when including more than just 2 features in the model. 
In case of Naive Bayes, this hints that the assumption of
independent feature values is actually too simple. 


The prediction probabilities are most extreme for Naive Bayes. 
In `r round(1000*mean(pred[,2] > 0.9 | pred[,2] < 0.1))/10` percent of the cases,
the predicted probability is at least 0.9 (or at most 0.1). Logistic regression is a little
less certain with `r round(1000*mean(lr.prob > 0.9 | lr.prob < 0.1))/10` percent. Logistic regression on 2 features has very low predicted probabilities. Exactly Now, `r round(1000*mean(lr2.prob > 0.9 | lr.prob < 0.1))/10` cases are predicted with certainty 0.9. percent of the cases is
predicted with high certainty. This is in correspondence with the much higher error rate of this method. 
Can we interpret these probabilities as an estimate of the error rate?



### 6. Penalized logistic regression
 
```{r glmnet, include=FALSE}
install.packages("glmnet", repos = "http://cran.xl-mirror.nl/")
library(glmnet)
``` 
We use the package `glmnet` for penalized regression. First, we plot the coefficients
versus the penalization factor $\lambda$, and estimate the optimal $\lambda$. 
by cross validation. 
```{r penLR}
# Convert feature values to numeric 
x <- sapply(Train[, -1], as.numeric) # N == 2, Y == 1.  
y <- Train[,1]
# Perform regression:
penalizedLR <-  glmnet(x, y, family = "binomial")
plot(penalizedLR, xvar = "lambda", label = TRUE)
# Use cross validation to estimate best lambda
cvPenalizedLR<-cv.glmnet(x, y, family="binomial", nfolds=10)
#plot(cvPenalizedLR)
```

Which features are included in the final model? Most of them, e.g. `physician` and  `adoption` 
have a high relative difference in votes, as displayed in the figure and list from the introduction.
Others, such as `crime`, do not seperate the two parties as neatly. Apparently, they
are useful in seperating the two classes when combined with the other 'non-zero featues'.
```{r features}
coef(cvPenalizedLR)
``` 

Finally, we use the optimal lambda to make a prediction. 
```{r penLR2}
xTest <-  sapply(Test[, -1], as.numeric) 
pred  <- predict(penalizedLR, xTest)
bestpred <- pred[ ,which(cvPenalizedLR$lambda==cvPenalizedLR$lambda.min)]
bestCharPred <- rep("democrat", length(bestpred))
bestCharPred[bestpred > 0] <- "republican"
View(bestCharPred)
```

The error on the test set equals Error rate: ` tail(lossPenalizedLR, n = 1)`.  This is an improvement
over both Naive Bayes and non-penalized logistic regression. 
```{r penLR3, warning=FALSE}
tryGetPenalizedLoss <- function(x, y) {
  penalizedLR <-  glmnet(x, y, family = "binomial")
  cvPenalizedLR<-cv.glmnet(x, y, family="binomial", nfolds=10)
  pred  <- predict(penalizedLR, xTest)
  bestpred <- pred[ ,which(cvPenalizedLR$lambda==cvPenalizedLR$lambda.min)]
  bestCharPred <- rep("democrat", length(bestpred))
  bestCharPred[bestpred > 0] <- "republican"
  mean(bestCharPred != Test[, 1])
}
getPenalizedLRloss <- function(x, y) {
  lossLR <- rep(1, dim(x)[1])
  for(n in 1:218) { 
    xTemp <- x[1:n, ]
    yTemp <- y[1:n]
    lossLR[n] <- tryCatch(tryGetPenalizedLoss(xTemp, yTemp), error = function(c) 1)
  }
  return(lossLR)
}
x <- sapply(Train[, -1], as.numeric) # N == 2, Y == 1.  
y <- Train[,1]
xTest <-  sapply(Test[, -1], as.numeric) 
lossPenalizedLR <- getPenalizedLRloss(x, y)
```

The error on the test set equals Error rate: `r tail(lossPenalizedLR, n = 1)`.  This is an improvement
over both Naive Bayes and non-penalized logistic regression. 
Plotting the test error shows that this methods performance behaves somewhat
erradic, because we did not perform multiple runs as for the other methods.
The real advantage of penalizing shows when looking at the performance for different
sample sizes. Even for very small sample sizes,  we are very close to
the 'asymptotic' error rate! Hence, penalized logistic regression
provides the best predictions, it does this also much, much faster than 
Naive Bayes. This indicates that this method should be preferred
also for small sample sizes. 
 

```{r TestcomparisonPlot2, cache=TRUE}
plotPerformance <- function(meanPerformance) {
  plot(c(0, 220), c(0, 0.6), type = 'n', 
       xlab = "Size of training set", ylab = "mean 0/1-loss",)
  colors = c("black", "blue", "red", "yellow", "orange")
  for(i in 3:5) { # 1 time NB
    lines(meanPerformance[, i], type = 'l', col = colors[i-1])
  }
  lines(lossPenalizedLR, type = 'l', col = colors[5])
  legend(150, 0.6, c("NB 1",  "LG", "LG 2", "Penalized LR"), lty=rep(1, 4), 
         lwd=rep(2.5, 4), col=colors[-1])
}
plotPerformance(TestPerformance)
title(main = 'Test errors')
```
